{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4cbeed",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef003d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "import math\n",
    "\n",
    "import datasus_dbc # Used to decompress the DBC file from DATASUS\n",
    "import dbfread # Used to read the decompressed DBF\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Image\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"my_geocoder_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbfe15be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the option to display all columns in the DataFrame\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f818f97f",
   "metadata": {},
   "source": [
    "# 1. Class RT_APAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1d03bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RT_APAC:\n",
    "\n",
    "    # Basic imports\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    import shutil\n",
    "    from geopy.geocoders import Nominatim\n",
    "\n",
    "    import math\n",
    "\n",
    "    import datasus_dbc # Used to decompress the DBC file from DATASUS\n",
    "    import dbfread # Used to read the decompressed DBF\n",
    "\n",
    "    from IPython.core.display import HTML\n",
    "    from IPython.display import Image\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"my_geocoder_app\")\n",
    "\n",
    "    # organizing files from datasus\n",
    "    # orginal_path: path to the directory where the files are located\n",
    "    # state: two-letter abbreviation of the state (e.g., 'sp' for São Paulo\n",
    "    def organize_files(original_path:str, state:str, start_year: 8, end_year: int = 25, file_structure: str = 'AR', type: str = '.dbc'):\n",
    "        print('')\n",
    "        print(original_path)\n",
    "        print('')\n",
    "        state = state.upper()\n",
    "        file_structure = file_structure + state\n",
    "        type = '.dbc'\n",
    "\n",
    "        # creating list with years\n",
    "        years = []\n",
    "        for y in range(start_year, end_year):\n",
    "            if y < 10:\n",
    "                years.append('0' + str(y))\n",
    "            else:\n",
    "                years.append(str(y))\n",
    "        # creating list with months\n",
    "        months = []\n",
    "        for m in range(1,13):\n",
    "            if m < 10:\n",
    "                months.append('0' + str(m))\n",
    "            else:\n",
    "                months.append(str(m))\n",
    "\n",
    "        # creating directory for each year\n",
    "        for y in years:\n",
    "            path = os.path.join(original_path, y)        \n",
    "            # creating directory if it does not exist\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "                print(f\"Created directory: {y}\")\n",
    "            else:\n",
    "                print(f\"Directory already exists: {y}\")\n",
    "\n",
    "        # checking for files and moving them to the corresponding directory\n",
    "        for y in years:\n",
    "            path = os.path.join(original_path, y)\n",
    "            # creating directory for each month\n",
    "            for m in months:\n",
    "                file_name = f\"{file_structure}{y}{m}{type}\"\n",
    "                file_name = os.path.join(original_path, file_name)\n",
    "                # print(f\"Checking for file: {file_name}\")                        \n",
    "                # moving file to the corresponding directory\n",
    "                if os.path.exists(file_name):\n",
    "                    # print(f\"File found: {file_name}\")                     \n",
    "                    shutil.move(file_name, path)                \n",
    "                    # print(f\"Moved {file_name} to {y}/\")\n",
    "      \n",
    "    def concat_all_data(path_files, state): # Final dictionary to hold the results  \n",
    "\n",
    "        # Create a full data DataFrame to hold all data\n",
    "        full_data = pd.DataFrame()            \n",
    "        \n",
    "        # Path to the directory containing the DBC files\n",
    "        path_files = os.path.join(path_files, state)  # Adjust the path as needed\n",
    "\n",
    "        # Check all paths in the directory\n",
    "        for path in os.listdir(path_files):            \n",
    "\n",
    "            # Create an empty DataFrame to hold all data\n",
    "            all_data = pd.DataFrame()\n",
    "\n",
    "            # Print the path being processed\n",
    "            print(\"Processing path:\", path, end=' from -> ')            \n",
    "\n",
    "            # Construct the full path to the directory            \n",
    "            path_destiny = os.path.join(path_files, path)\n",
    "            print(path_destiny)            \n",
    "        \n",
    "            # Check all files in the path\n",
    "            try:\n",
    "                for file_to_read in os.listdir(path_destiny):                \n",
    "                \n",
    "                    # Construct the full path to the file\n",
    "                    file_to_read = os.path.join(path_destiny, file_to_read)\n",
    "                    # print(\"Reading file:\", file_to_read)   \n",
    "                \n",
    "                    # Decompress the DBC file to a temporary DBF file\n",
    "                    datasus_dbc.decompress(file_to_read, \"output.dbf\")\n",
    "\n",
    "                    # Read the decompressed DBF file using dbfread\n",
    "                    dbf = dbfread.DBF('output.dbf', encoding='latin1')\n",
    "                    #df = pd.DataFrame(iter(dbf))\n",
    "\n",
    "                    # Now you can work with the DataFrame\n",
    "                    df = pd.DataFrame(iter(dbf))  # Convert to DataFrame\n",
    "\n",
    "                    for col in df.columns:\n",
    "                        # Convert columns to string type\n",
    "                        df[col] = df[col].astype(str)                \n",
    "\n",
    "                    # Concatenate the DataFrame to the main DataFrame                \n",
    "                    all_data = pd.concat([all_data, df], ignore_index=True)            \n",
    "                \n",
    "                    # Construct the full path to save the CSV file\n",
    "                    csv_destiny = os.path.join(path_files, f\"{path}.csv\") \n",
    "            except NotADirectoryError:\n",
    "                print(f\"Error: {path_destiny} is not a directory or does not exist.\")\n",
    "                pass               \n",
    "        \n",
    "            # Concatenate the all_data DataFrame to the full_data DataFrame\n",
    "            full_data = pd.concat([full_data, all_data], ignore_index=True)\n",
    "        \n",
    "\n",
    "        # Save the full_data DataFrame to a CSV file\n",
    "        csv_destiny = os.path.join(path_files, f\"{state}.csv\")\n",
    "        full_data.to_csv(csv_destiny, index=False)\n",
    "\n",
    "    # Extracting lat long from cities\n",
    "    def lat_long(dataset, state_country):\n",
    "        # Using api to extract latitude and longitude\n",
    "        lat_long_dict = {}\n",
    "\n",
    "        state_country = \", Paraná, BR\"\n",
    "\n",
    "        for city in dataset['cidade']:\n",
    "            city_name = city\n",
    "            location = geolocator.geocode(city_name + state_country)\n",
    "            if location:\n",
    "                latitude = location.latitude\n",
    "                longitude = location.longitude\n",
    "                lat_long_dict[city_name] = (latitude, longitude)        \n",
    "            else:\n",
    "                print(f\"Could not find coordinates for {city_name}\")\n",
    "                lat_long_dict[city_name] = (None, None)\n",
    "        return lat_long_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d436c9e6",
   "metadata": {},
   "source": [
    "# 2. Running functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a54a04a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\175 MX\\Documents\\Gustavo\\datasus\\data_rt_states\\states\\pr\\qt\\dbcs\n",
      "\n",
      "Created directory: 08\n",
      "Created directory: 09\n",
      "Created directory: 10\n",
      "Created directory: 11\n",
      "Created directory: 12\n",
      "Created directory: 13\n",
      "Created directory: 14\n",
      "Created directory: 15\n",
      "Created directory: 16\n",
      "Created directory: 17\n",
      "Created directory: 18\n",
      "Created directory: 19\n",
      "Created directory: 20\n",
      "Created directory: 21\n",
      "Created directory: 22\n",
      "Created directory: 23\n",
      "Created directory: 24\n"
     ]
    }
   ],
   "source": [
    "organize = RT_APAC.organize_files('C:\\\\Users\\\\175 MX\\\\Documents\\\\Gustavo\\\\datasus\\\\data_rt_states\\\\states\\\\pr\\\\qt\\\\dbcs', 'pr', 8, 25, file_structure='AQ', type='.dbc')\n",
    "organize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "113d1ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing path: 08 from -> states\\pr\\qt\\dbcs\\08\n",
      "Processing path: 09 from -> states\\pr\\qt\\dbcs\\09\n",
      "Processing path: 10 from -> states\\pr\\qt\\dbcs\\10\n",
      "Processing path: 11 from -> states\\pr\\qt\\dbcs\\11\n",
      "Processing path: 12 from -> states\\pr\\qt\\dbcs\\12\n",
      "Processing path: 13 from -> states\\pr\\qt\\dbcs\\13\n",
      "Processing path: 14 from -> states\\pr\\qt\\dbcs\\14\n",
      "Processing path: 15 from -> states\\pr\\qt\\dbcs\\15\n",
      "Processing path: 16 from -> states\\pr\\qt\\dbcs\\16\n",
      "Processing path: 17 from -> states\\pr\\qt\\dbcs\\17\n",
      "Processing path: 18 from -> states\\pr\\qt\\dbcs\\18\n",
      "Processing path: 19 from -> states\\pr\\qt\\dbcs\\19\n",
      "Processing path: 20 from -> states\\pr\\qt\\dbcs\\20\n",
      "Processing path: 21 from -> states\\pr\\qt\\dbcs\\21\n",
      "Processing path: 22 from -> states\\pr\\qt\\dbcs\\22\n",
      "Processing path: 23 from -> states\\pr\\qt\\dbcs\\23\n",
      "Processing path: 24 from -> states\\pr\\qt\\dbcs\\24\n"
     ]
    }
   ],
   "source": [
    "data = RT_APAC.concat_all_data(\"states\\\\pr\\\\qt\", \"dbcs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
